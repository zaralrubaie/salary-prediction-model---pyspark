# -*- coding: utf-8 -*-
"""SALARY PREDICTION BY PYSPARK

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/133W2L-2sCVxfy-ZjIZkrMkbxoGZg16la
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col,sum,avg
spark=SparkSession.builder.appName('salary prediction').getOrCreate()
df=spark.read.csv('/content/ds_salaries.csv',header=True, inferSchema=True)
df.show()

df.describe().show()

df.select([sum(col(c).isNull().cast("int")).alias(c) for c in df.columns]).show()

df.show(3)

#feature exraction
from pyspark.sql.functions import when

df = df.withColumn(
    "experience_level_num",
    when(df.experience_level == "EN", 0)  # Entry level
    .when(df.experience_level == "MI", 1) # Mid level
    .when(df.experience_level == "SE", 2) # Senior level
    .when(df.experience_level == "EX", 3) # Executive

)

df = df.withColumn(
    "remote_type",
    when(df.remote_ratio == 100, "Fully Remote")
    .when((df.remote_ratio > 0) & (df.remote_ratio < 100), "Hybrid")
    .otherwise("Onsite")
)
df = df.withColumn(
    "company_size_num",
    when(df.company_size == "S", 0)
    .when(df.company_size == "M", 1)
    .when(df.company_size == "L", 2)
)

df = df.withColumn("salary_diff_usd", df.salary_in_usd - df.salary)

avg_salary_by_job = df.groupBy("job_title").agg(avg("salary").alias("avg_salary_by_title"))
df = df.join(avg_salary_by_job, on="job_title", how="left")
df = df.withColumn("work_year", df["work_year"].cast("int"))

df=df.drop('experience_level','company_size')

df.show()

from pyspark.sql.functions import skewness, col

numeric_cols = ["work_year", "remote_ratio", "experience_level_num", "company_size_num", "salary_in_usd",'avg_salary_by_title','salary_diff_usd','salary']

print("Skewness of numeric columns:")

for c in numeric_cols:
    skew_val = df.select(skewness(col(c))).collect()[0][0]
    print(f"{c}: {skew_val}")

from pyspark.sql.functions import log1p, col
from pyspark.sql import functions as F

skewed_cols = ["avg_salary_by_title", "salary_diff_usd", "salary"]

# Apply log1p (log(x + 1)) transformation
for c in skewed_cols:
    df = df.withColumn(c + "_log", log1p(col(c).cast("double")))

# Function to compute skewness for a column
def compute_skewness(df, col_name):
    stats = df.select(F.skewness(col_name)).collect()[0][0]
    return stats

# Check skewness before and after log transformation
for c in skewed_cols:
    original_skew = compute_skewness(df, c)
    log_skew = compute_skewness(df, c + "_log")
    print(f"Column: {c} | Original Skewness: {original_skew:.4f} | Log-Transformed Skewness: {log_skew:.4f}")

df =df.drop(
    'salary',
    'salary_in_usd',
    'salary_diff_usd',
    'avg_salary_by_title',
    'salary_diff_usd_log',
    'avg_salary_by_title_log')  #

df.show()

from pyspark.ml.feature import StringIndexer

categorical_cols = [
    "job_title",
    "employment_type",
    "salary_currency",
    "employee_residence",
    "remote_type"
]

for col in categorical_cols:
    indexer = StringIndexer(inputCol=col, outputCol=col + "_index", handleInvalid="keep")
    df = indexer.fit(df).transform(df)

df=df.drop( "job_title",
    "employment_type",
    "salary_currency",
    "employee_residence",
    "remote_type",'company_location')

from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import DecisionTreeRegressor
from pyspark.ml.evaluation import RegressionEvaluator

# Columns you have (numeric + encoded categorical)
feature_cols = [
    'work_year',
    'remote_ratio',
    'experience_level_num',
    'company_size_num',
    'job_title_index',
    'employment_type_index',
    'salary_currency_index',
    'employee_residence_index',
    'remote_type_index'
]

target_col = 'salary_log'  # your target (log scale)

# 1. Split the data first
train_data, test_data = df.randomSplit([0.7, 0.3], seed=42)

# 2. Drop 'features' column if exists (to avoid duplication issues)
if 'features' in train_data.columns:
    train_data = train_data.drop('features')

if 'features' in test_data.columns:
    test_data = test_data.drop('features')

# 3. VectorAssembler to combine features
assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')
train_data = assembler.transform(train_data)
test_data = assembler.transform(test_data)

# 4. Initialize Decision Tree Regressor
dt = DecisionTreeRegressor(featuresCol='features', labelCol=target_col, maxBins=100)  # maxBins > max categories

model = dt.fit(train_data)

predictions = model.transform(test_data)

evaluator_rmse = RegressionEvaluator(labelCol=target_col, predictionCol='prediction', metricName='rmse')
evaluator_r2 = RegressionEvaluator(labelCol=target_col, predictionCol='prediction', metricName='r2')

rmse = evaluator_rmse.evaluate(predictions)
r2 = evaluator_r2.evaluate(predictions)

print(f"RMSE: {rmse}")
print(f"R2: {r2}")

df.write.option("header", True).csv("clean_salary_data.csv")